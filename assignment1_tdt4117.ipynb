{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00a8eb",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 1: Boolean Model, TF-IDF, and Data Retrieval vs. Information Retrieval Conceptual Questions\n",
    "\n",
    "**Student names**: Subaial Awan & Las Alei<br>\n",
    "**Group number**: 79 <br>\n",
    "**Date**: _Submission Date_\n",
    "\n",
    "## Important notes\n",
    "Please carefully read the following notes and consider them for the assignment delivery. Submissions that do not fulfill these requirements will not be assessed and should be submitted again.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. The assignment must be delivered in ipynb format.\n",
    "3. The assignment must be typed. Handwritten assignments are not accepted.\n",
    "\n",
    "**Due date**: 14.09.2025 23:59\n",
    "\n",
    "In this assignment, you will:\n",
    "- Implement a Boolean retrieval model\n",
    "- Compute TF-IDF vectors for documents\n",
    "- Run retrieval on queries\n",
    "- Answer conceptual questions \n",
    "\n",
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready — just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3249058",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773d293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"/Users/subaialawan/Downloads/Assignment 1 3/cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = parse_cranfield(CRAN_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8f900",
   "metadata": {},
   "source": [
    "## 1.1 – Boolean Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81516f89",
   "metadata": {},
   "source": [
    "### 1.1.1 Tokenize documents\n",
    "\n",
    "Implement tokenization using the given list of stopwords. Create a list of normalized terms per document (e.g., lowercase, remove punctuation/digits; drop stopwords). Store the token lists to use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d78a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement tokenization using the given list of stopwords, create list of terms per document\n",
    "import re\n",
    "import string\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "\n",
    "def tokenization(docs):\n",
    "  \n",
    "    result = []\n",
    "    \n",
    "    for doc in docs.values():\n",
    "        text = doc[\"title\"] + \" \" + doc[\"abstract\"]\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        \n",
    "        tokens = text.split()\n",
    "        \n",
    "        filtered_tokens = [token for token in tokens if token not in STOPWORDS and token.strip()]\n",
    "        \n",
    "        result.append(filtered_tokens)\n",
    "    \n",
    "    return result\n",
    "\n",
    "tokenized_documents = tokenization(docs) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183f40d",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "\n",
    "Create a set (or list) of unique terms from all tokenized documents. Report the number of unique terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa9cc192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 7361\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a set or list of unique terms\n",
    "\n",
    "# Report: \n",
    "# - Number of unique terms\n",
    "\n",
    "# Your code here\n",
    "\n",
    "def build_vocabulary(tokenized_documents):\n",
    "   \n",
    "    vocabulary = set()\n",
    "    \n",
    "    for document_tokens in tokenized_documents:\n",
    "        vocabulary.update(document_tokens)\n",
    "        \n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(tokenized_documents)  \n",
    "print(f\"Number of unique terms: {len(vocabulary)}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb02912",
   "metadata": {},
   "source": [
    "### Build inverted index\n",
    "\n",
    "For each term, store the list (or set) of document IDs where the term appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "393b2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: For each term, store list of document IDs where the term appears\n",
    "# Your code here\n",
    "def build_inverted_index(tokenized_documents):\n",
    "  \n",
    "    inverted_index = {}  \n",
    "    \n",
    "    for doc_id, document_tokens in enumerate(tokenized_documents):\n",
    "        for term in document_tokens:\n",
    "            if term not in inverted_index:\n",
    "                inverted_index[term] = set()\n",
    "            \n",
    "            inverted_index[term].add(doc_id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = build_inverted_index(tokenized_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e81bf",
   "metadata": {},
   "source": [
    "### Retrieve documents for a Boolean query (AND/OR)\n",
    "\n",
    "Create a function to retrieve documents for a Boolean query (AND/OR) with query terms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d9c9318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function for retrieving documents for a Boolean query (AND/OR) with query terms\n",
    "\n",
    "def boolean_retrieve(query: str):\n",
    "   \n",
    "    query = query.lower().strip()\n",
    "    \n",
    "    if not query:\n",
    "        return []\n",
    "    \n",
    "    tokens = query.split()\n",
    "    \n",
    "    filtered_tokens = [token for token in tokens if token in ['and', 'or'] or token not in STOPWORDS and token.strip()]\n",
    "    \n",
    "    if not filtered_tokens:\n",
    "        return []\n",
    "    \n",
    "    or_groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for token in filtered_tokens:\n",
    "        if token == 'or':\n",
    "            if current_group:\n",
    "                or_groups.append(current_group)\n",
    "            current_group = []\n",
    "        else:\n",
    "            current_group.append(token)\n",
    "    \n",
    "    if current_group:\n",
    "        or_groups.append(current_group)\n",
    "    \n",
    "    final_results = set()\n",
    "    \n",
    "    for group in or_groups:\n",
    "        terms = [token for token in group if token != 'and']\n",
    "        \n",
    "        if not terms:\n",
    "            continue\n",
    "        \n",
    "        group_result = inverted_index.get(terms[0], set()).copy()\n",
    "        \n",
    "        for term in terms[1:]:\n",
    "            term_docs = inverted_index.get(term, set())\n",
    "            group_result &= term_docs  \n",
    "        \n",
    "        final_results |= group_result\n",
    "    \n",
    "    return sorted(final_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7bf47585",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "boolean_queries = [\n",
    "  \"gas AND pressure\",\n",
    "  \"structural AND aeroelastic AND flight AND high AND speed OR aircraft\",\n",
    "  \"heat AND conduction AND composite AND slabs\",\n",
    "  \"boundary AND layer AND control\",\n",
    "  \"compressible AND flow AND nozzle\",\n",
    "  \"combustion AND chamber AND injection\",\n",
    "  \"laminar AND turbulent AND transition\",\n",
    "  \"fatigue AND crack AND growth\",\n",
    "  \"wing AND tip AND vortices\",\n",
    "  \"propulsion AND efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eaf286d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 => [26, 48, 84, 100, 109]\n",
      "Query 2 => [11, 13, 28, 46, 50]\n",
      "Query 3 => [4, 398]\n",
      "Query 4 => [0, 60, 243, 264, 341]\n",
      "Query 5 => [117, 130]\n",
      "Query 6 => []\n",
      "Query 7 => [6, 8, 79, 88, 95]\n",
      "Query 8 => []\n",
      "Query 9 => [674]\n",
      "Query 10 => [967]\n"
     ]
    }
   ],
   "source": [
    "# Run Boolean queries in batch, using the function you created\n",
    "def run_batch_boolean(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = boolean_retrieve(q)\n",
    "        results[f\"Query {i}\"] = res\n",
    "    return results\n",
    "\n",
    "boolean_results = run_batch_boolean(boolean_queries)\n",
    "for qid, res in boolean_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b591b81",
   "metadata": {},
   "source": [
    "## Part 1.2 – TF-IDF Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed448f",
   "metadata": {},
   "source": [
    "\n",
    "$tf_{i,j} = \\text{Raw Frequency}$\n",
    "\n",
    "$idf_t = \\log\\left(\\frac{N}{df_t}\\right)$\n",
    "\n",
    "### Build document–term matrix (TF and IDF weights)\n",
    "\n",
    "Compute tf and idf using the formulas above and store the weights in a document–term matrix (rows = documents, columns = terms).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "629e32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the weights for the documents and the terms using tf and idf weighting. Put these values into a document–term matrix (rows = documents, columns = terms).\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_tf_idf_matrix():\n",
    "    \n",
    "    terms_list = sorted(list(vocabulary))  \n",
    "    num_docs = len(tokenized_documents)  \n",
    "    num_terms = len(terms_list)  \n",
    "    \n",
    "    term_to_idx = {term: idx for idx, term in enumerate(terms_list)}\n",
    "    \n",
    "    tf_idf_matrix = np.zeros((num_docs, num_terms))\n",
    "    \n",
    "    for doc_idx, doc_tokens in enumerate(tokenized_documents):\n",
    "        term_counts = defaultdict(int)\n",
    "        for token in doc_tokens:\n",
    "            term_counts[token] += 1\n",
    "        \n",
    "        for term, count in term_counts.items():\n",
    "            if term in term_to_idx:\n",
    "                term_idx = term_to_idx[term]\n",
    "                \n",
    "                tf = count\n",
    "                \n",
    "                df_t = len(inverted_index.get(term, []))  \n",
    "                idf = math.log(num_docs / (df_t + 1))  \n",
    "                \n",
    "                tf_idf = tf * idf\n",
    "                tf_idf_matrix[doc_idx, term_idx] = tf_idf\n",
    "    \n",
    "    return tf_idf_matrix, terms_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007cbf7",
   "metadata": {},
   "source": [
    "### Build TF–IDF document vectors\n",
    "\n",
    "From the matrix, build a TF–IDF vector for each document (consider normalization if needed for cosine similarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "654b0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Build TF–IDF document vectors from the document–term matrix\n",
    "# Your code here\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_document_vectors(tf_idf_matrix, normalize=True):\n",
    "    document_vectors = tf_idf_matrix.copy()\n",
    "    \n",
    "    if normalize:\n",
    "        norms = np.linalg.norm(document_vectors, axis=1, keepdims=True)\n",
    "        norms = np.where(norms == 0, 1, norms)\n",
    "        document_vectors = document_vectors / norms\n",
    "    \n",
    "    return document_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df320c",
   "metadata": {},
   "source": [
    "### Implement cosine similarity\n",
    "\n",
    "Implement a function to compute cosine similarity scores between a (tokenized) query and all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44d2e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function for calculating the similarity score of all the documents by their relevance to query terms\n",
    "\n",
    "tf_idf_matrix, terms_list = calculate_tf_idf_matrix()\n",
    "document_vectors = build_document_vectors(tf_idf_matrix, normalize=True)\n",
    "\n",
    "\n",
    "def tfidf_retrieve(query: str, k=None):\n",
    "    query_lower = query.lower()\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    query_clean = query_lower.translate(translator)\n",
    "    query_tokens = [token for token in query_clean.split() \n",
    "                   if token not in STOPWORDS and token.strip()]\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    term_to_idx = {term: idx for idx, term in enumerate(terms_list)}\n",
    "    num_terms = len(terms_list)\n",
    "    num_docs = len(tokenized_documents)\n",
    "    \n",
    "    query_vector = np.zeros(num_terms)\n",
    "    query_term_counts = Counter(query_tokens)\n",
    "    \n",
    "    for term, tf in query_term_counts.items():\n",
    "        if term in term_to_idx:\n",
    "            term_idx = term_to_idx[term]\n",
    "            \n",
    "            if term in inverted_index:\n",
    "                df_t = len(inverted_index[term])\n",
    "                idf = math.log(num_docs / df_t)\n",
    "                \n",
    "                query_vector[term_idx] = tf * idf\n",
    "    \n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    if query_norm > 0:\n",
    "        query_vector = query_vector / query_norm\n",
    "    else:\n",
    "        return []  \n",
    "    \n",
    "    similarity_scores = np.dot(document_vectors, query_vector)\n",
    "    \n",
    "    ranked_indices = np.argsort(similarity_scores)[::-1]\n",
    "    \n",
    "    ranked_docs = []\n",
    "    for idx in ranked_indices:\n",
    "        if similarity_scores[idx] > 0:\n",
    "            ranked_docs.append(int(idx))\n",
    "        else:\n",
    "            break  \n",
    "    \n",
    "    if k is not None:\n",
    "        ranked_docs = ranked_docs[:k]\n",
    "    \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d4968750",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "tfidf_queries = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "18861681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [168, 1285, 166, 184, 165]\n",
      "Q2 => [11, 50, 745, 874, 883]\n",
      "Q3 => [398, 143, 484, 4, 180]\n",
      "Q4 => [367, 637, 450, 1162, 1348]\n",
      "Q5 => [388, 1186, 117, 171, 172]\n",
      "Q6 => [973, 627, 396, 307, 634]\n",
      "Q7 => [417, 1263, 314, 271, 1277]\n",
      "Q8 => [767, 725, 1195, 882, 883]\n",
      "Q9 => [1283, 432, 1270, 674, 287]\n",
      "Q10 => [967, 1327, 1379, 1091, 591]\n"
     ]
    }
   ],
   "source": [
    "# Run TF-IDF queries in batch (print top-5 results for each), using the function you created\n",
    "def run_batch_tfidf(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = tfidf_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "tfidf_results = run_batch_tfidf(tfidf_queries)\n",
    "\n",
    "for qid, res in tfidf_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0989101",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1.3 – Conceptual Questions\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "**1. What is the difference between data retrieval and information retrieval?**\n",
    "Data retrieval deals with structured data and retrieves data based specific queries, where information retrieval deals with unstructured data and retrieves relevant information from a large set of documents.\n",
    "\n",
    "**For the following scenarios, which approach would be suitable data retrieval or information retrieval? Explain your reasoning.** <br>\n",
    "\n",
    "1.a A clerk in pharmacy uses the following query: Medicine_name = Ibuprofen_400mg\n",
    "\n",
    "Since the query is looking for spesific structured data (Medicine_name = Ibuprofen_400mg), data retrieval would be the most suitable approach.\n",
    "\n",
    "1.b A clerk in pharmacy uses the following query: An anti-biotic medicine \n",
    "\n",
    "For this scenario, information retrieval would be more suitable because of \"An anti-biotic medicine\" is not something spesific but rather a broad thing. This is more a category, which the system would return a list of relevant medicines that match the description.\n",
    "\n",
    "1.c Searching for the schedule of a flight using the following query: Flight_ID = ZEFV2\n",
    "\n",
    "Data retrieval. Its looking for exact structured data Flight_ID = ZEFV2.\n",
    "\n",
    "1.d Searching an E-commerce website using the following query to find an specific shoe: Brooks Ghost 15\n",
    "\n",
    "Here information retrieval would be more suitable. \"Brooks Ghost 15\" is a specific search, but the system would have to search through a large collection of unstructured and semi-structured data like images, descriptions etc.\n",
    "\n",
    "1.e Searching the same E-commerce website using the following query: Nice running shoes\n",
    "\n",
    "Information retrieval. Nice running shoes is general item and not a specific one.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
